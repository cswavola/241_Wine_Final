---
title: "W241 Final Report"
subtitle: 'Vous voulez du vin? - Does the use of foreign language advertising in wine increase customer’s purchase likelihood?'
author: Rory Liu, Charlotte Swavola, Sharad Varadarajan
output:
  pdf_document: default
  html_document: default
fontsize: 11pt  
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath("./"))
```

## Introduction

When walking through the grocery store, one can easily spot products with foreign-language labels - “Vins de France” on a wine bottle, “Wirklich gut!” on a sausage pack, etc. Advertisers often confront consumers with foreign languages, such as German or French [^1]. In fact, the use of foreign language in advertising is a well-studied topic. According to the study done by France Leclerc, Bernd Schmitt and Laurette Dubé,showing French pronunciation of a brand name affects the perceived hedonism of the products, attitudes toward the brand, and attitudes toward the brand name[^2]. Furthermore, Jos Hornikx and Frank van meurs in their study that the use of foreign languages serve as a strong cue for a product's country of origin, and the associations that the foreign language evoke and those that the country-of-origin evoke are similar[^3].

[^1]: Jos Hornikx, Frank van Meurs & Robert-Jan Hof (2013) The Effectiveness of Foreign-Language Display in Advertising for Congruent versus Incongruent Products, Journal of International Consumer Marketing, 25:3, 152-165, DOI: 10.1080/08961530.2013.780451

[^2]: Leclerc, F., B. H. Schmitt, and L. Dubé. 1994. Foreign branding and its effects on product perceptions and attitudes. Journal of Marketing Research 31 (2):263–270

[^3]: Jos Hornikx & Frank van Meurs (2017) Foreign Languages in Advertising as Implicit Country-of-Origin Cues: Mechanism, Associations, and Effectiveness, Journal of International Consumer Marketing, 29:2, 60-73, DOI: 10.1080/08961530.2016.1243996

While scholars seem to agree that foreign language advertising shapes product perception by implicitly giving consumers cues on product's country of origin, few of the existing studies dived deep into the causal link between foreign language and consumer’s purchase likelihood. This is an important causal link to establish. At the end of the day, the success of an advertising campaign is, or should be measured by the additional sales the campaign generates. Without understanding of whether having foreign language in product communications increases sales, advertiser will be ill-guided in their decision to apply such advertising techniques.

In our study, we seek to understand the relationship between foreign language advertising and consumer willingness to pay. To narrow down our scope for a manageable experiement given our time and resource constaints, we decided to focus on one particular product: wine. The high variance of wine prices, and the strong association between place of origin and the signal on quality makes it a great subject for our study.

## Experimental Design
To fully understand the relationship between foreign language advertising and will purchases, we drafted a multi-factorial design for our experiement. There are three main factors that we want to investigate: advertising language, wine's country of origin, and length of flavor profile description.

For advertising language, we will test 3 languages: English (as baseline), French and German. We selected French because France is typically perceived as a premium wine production country, and we hypothesize that advertsizing in French will increase consumer's willingness to pay. We also selected German as a comparison language. Germany is not typically associated with wine, and therefore, by including German, we will be able to see whether the effect of having foreign language is limited to the language of the country associated with a product, or whether it expands to other languages as well.

For wine's country of origin, we included US and France. Here we are particularly interested in the interaction between country of origin and advertsing language. Prior studies have found that foreign-language advertising serve as country of origin cues. By including country of origin as a variable, we are able to see whether foreign language has any additional impact when country of origin is also given.

For flavor profile description, we have two versions - long and short. We believe that this could be a proxy for foreign language "dosage". A long flavor profile in French might be more noticeable than a short French tagline, and may amplify the effect that we find.

To summarize, we have a 3 x 2 x 2 design, which results in 12 groups of participants. (See summary in figure 1).

![Groups Design](Group_Design.png)


Because of our time and resource constraint, we could not conduct a real-life experiement with actual wines in actual stores. Instead, we decided to conduct the study through a Qualtrics survey and gather responses through Mechanical Turk. The survey is structured into the following sections:

![Survey Flow](survey_flow.png)


We start with some demographics question about participants primary language, income and gender. These will serve as covariates in our later model. Then, we ask participants about their wine-related behavior, including how often they drink wine, whether they like Cabernet Sauvignon, and how often they purchase bottles of wine. We think that these wine-related behavior will explain some of the variation in willingness-to-pay, and therefore will be good covariates that help us reduce our standard error. The third section is the main part of our experiement where we introduce the treament. Here, we randomly assign the participants into one of twelve groups mentioned above, and show them a simulated purchasing page (example seen in figure 3).

![Page Sample](Sample_wine_page.png)

In the simulated wine page, we translate all texts into the groups assigned language, and provide english translation for key elements (e.g. flavor profile, 'top wine of the year' tag) to make sure that the respondent understand the information. All groups are shown the exact same wine (which was a hypothetical wine fabricated by us), and the same descriptive information. We made the decision to present the page entirely in French or German for related tracks because we wanted to make sure participants notice our language treatment.

We then ask the respondents to guess the cost of the wine. This is an un-anchored direct pricing question, aimed to understand respondents' first perception of the wine shown. Afterwards, we use a simplified version of the Gabor Granger method developed by André Gabor and C. W. J. Granger in the 1960s [^4]. In this method, we ask respondents whether they would consider purchase the shown wine at $50. If respondents say now, we ask the same question at a lower price point of $35. If they still answers no, we lower the price further to $20. Through this series of 3 questions, we seek to understand where their true willingness-to-pay lies. We selected the 3 price points to cover a wide range of potential willingness-to-pay, and validated our choice through a pilot study of 100 respondents. From the pilot, we saw that most respondents' willingness to pay fall into the 20-50 price range.

[^4]: Gabor, A. and Granger, C. (1966). Price as an Indicator of Quality: Report on an Enquiry. Economica, 33(129), p.43.

Our survey is concluded after the treatment section for our main sample of 1200 respondents. We used the data from this main sample to run various regressions and randomized inferences and seek to find results with statistical significance. 

However, to validate the findings from this main sample, we collected another (smaller) sample of 600 respondents. The data from this second batch of respondents served as our validation test. By running the key models we deemed important from main sample with this validation data set, we can confirm that the relationship we found is real.

In our second launch, we maintained the same survey experience by asking exactly the same questions in the same sequence. However, we did introduce a baseline question at the very end of the second survey, asking respondents to estimate the price of a gallon of milk. Because this question came at the very end of the survey, it should not interfere with our treatment, and would not create any difference between the data from first and second launch. However, having this baseline questions helps us establish a reference price points for respondents price guesses, and adjust for any overall inflated / deflated guesses.

## EDA and Data Cleansing  

After loading the data gathered from our survey, we first went through some general data processing and examined descriptive statistics to better understand our general results. The three figures below helped support the team in getting a high-level summary of the data. With regards to the distribution for our outcome variables, the team noticed considerable variance in respondent's price perception but that the large majority of guesses fell between 1-100 dollars. This finding greatly influenced our model building decisions. With regards to the covariate barplots below, the team was pleased to see a rather normal distribution for both purchase frequency and drink frequency. Additionally, we noticed that English was the primary language for almost all of our respondents, and that a menial number respondents were fluent in either German or French; these are encouraging statistics considering that we want to evaluate the effect of languages that are "foreign" to our users.  

```{r, include=FALSE}
d = read.csv('1st_Launch_Full.csv')
# load packages 
library(knitr)
library(data.table)
library(dplyr)
library(tidyr)
library(foreign)
library(lmtest)
library(sandwich)
library(multiwayvcov)
library(stargazer)
library(ggplot2)
library(MASS)
```

```{r, include=FALSE}
#Functions for later use
standard_conf_int_95 = function(regression, n){
  r = coeftest(regression, vcovHC(regression))
  upper_bound = r[n,1] + 1.96* r[n, 2] 
  lower_bound = r[n,1] -1.96* r[n, 2]
  return (c(lower_bound,upper_bound))
}
```

```{r, include=FALSE}
#Process/clean variables- covariates
d<-data.table(d)
#English as primary language
d$English_as_primary<-d[,.(abs(d$English_as_primary-2))]
d$English_as_primary<-as.factor(d$English_as_primary)
#Male: original, male=1, female=2
d$Male<-d[,.(abs(d$Male-2))]
d$Male<-as.factor(d$Male)
#household income- as factor due to <20k, >100k, and do not answer
d$Household_Income<-as.factor(d$Household_Income)
#Other languages- Blank is only English, French 1, German 2, Other 3
d$Speaks_German<-as.numeric(d[,Other_Language_Spoken %like% "2"])
d$Speaks_French<-as.numeric(d[,Other_Language_Spoken %like% "1"])

#Drink Frequency- options are not linear 
d$Drink_Frequency<-factor(d$Drink_Frequency)

#Cab preference
d$Cab_Preference<-d[,.(abs(d$Cab_Preference-2))]

#Purchase Frequency- also not linear
d$Purchase_Frequency<-factor(d$Purchase_Frequency)

d_factor<-d
```

```{r,include=FALSE}

#survey question datalabels. This does not work perfectly....
#to be used in demographic barplots, with name.arg()
English_as_primary<-c("No","Yes")
Male<-c("Female","Male")
Household_Income<-c("<$20,000","$20,000 to $34,999","$35,000 to $49,999",
                    "$50,000 to $74,999","$75,000 to $99,999",">$100,000","Prefer not to answer")
Speaks_German<-c("No","Yes")
Speaks_French<-c("No","Yes")
Drink_Frequency<-c(">Weekly","Weekly","Monthly","Seldom","Never")
Cab_Preference<-c("No","Yes")
Purchase_Frequency<-c(">Weekly","Weekly","Monthly","Seldom","Never")

demographics_labels<-list(English_as_primary,Male,Household_Income,Speaks_German,Speaks_French,Drink_Frequency,Cab_Preference,Purchase_Frequency)
demographics<-list(d$English_as_primary,d$Male,d$Household_Income,d$Speaks_German,d$Speaks_French,d$Drink_Frequency,d$Cab_Preference,d$Purchase_Frequency)

#demographics_labels[1]
```



```{r, echo = FALSE}
d<-data.table(d)
demographics=data.table(d[,.(English_as_primary,Male,Household_Income,Speaks_German,Speaks_French,Drink_Frequency,Cab_Preference,Purchase_Frequency)])
stargazer(d[,.(Guess_the_Cost,Purchase_50,Purchase_35,Purchase_20)],type="text",title = "Output variables, Pre-Cleaning")
stargazer(d[,.(English_as_primary,Male,Household_Income)], type = "text")
hist(d$Guess_the_Cost[d$Guess_the_Cost],main="Guess the cost, Pre-Cleaning",breaks=100,xlab= "Guessed cost")

```

```{r, echo = FALSE}
par(mfrow=c(3,3))
#look at randomization/tracks 
barplot(prop.table(table(d$Track)),main="Distribution across Tracks, Normalized",names.arg = c(-1:12))
#Important to note- 0 on barchart is N/A or attrition.
for (i in (1:length(names(demographics)))){
  #print(summary(demographics[,..i]))
  barplot(table(demographics[,..i]),
           main=names(demographics)[i]
          #,names.arg = demographics_labels[i]
          )}
```

After evaluating these high-level statistics, we proceeded to analyze more granular details regarding each submission and discovered the following areas that require attention: 

1). There were ~5% attrition (i.e. 5% of the respondents did not finish their survey). The attrition we see falls into 2 types: pre-treatmenet attrition and post-treatment attrition. For pre-treatment attrition, we proceeded to clean them out completely, because these respondents were not exposed to treatment yet, and their dropping off can't have anything to do with treatment we introduce. Dropping them will not create bias in our data. For post-treatment attrition, we wanted to be more careful. We will examine them later on to understand whether we have differential attrition based on treatment assigned.

2). There are ~1/3 of respondents that have duplicated IP address and/or geographical coordinates (latitude and longitude). We are worried about these responses because they might be repeat survey takers (that potentially got exposure to more than one treatment), or they might be bots or click farms that do not answer surveys seriously. We proceed to clean out all responses that does not have unique ip addresses and/or geographical coordinates.

3). As seen from the histogram above, we reiterate that we noticed many large outliers for guess-the-cost. The large variation in the answers will hurt our ability to obtain a reasonable standard error and find significant findings. We need to pay special attenion in our modeling phase later, to try to minize the effect of outliers.
  

```{r deduping, include = FALSE}
# Cleaning out incomplete responses before treament exposures
d <- d[!is.na(d$Track),]
#summary(d)

# Counting occurances of IP address and Geo-codes
d$geo_code <- paste(d$LocationLatitude, d$LocationLongitude)
dt = data.table(d)
dt[, `freq_geo` := .N, by = geo_code]
dt[, `freq_ip` := .N, by = IPAddress]
#class(as.data.frame(dt))
#dt
## Creatig dedupped dataset where we do not allow any duplicates in IP or Geo code

complete_dedup<- dt[dt$freq_ip==1 & (dt$freq_geo == 1 | (dt$geo_code == 'NA NA' & Track!= ''))]

d<-complete_dedup
```

After cleaning the data, we analyzed the previous descriptive statistics once more to see the effects. Below we see that cleaning reduced our outlier problem significantly for guess-the-cost, although considerable variation still exists. 

### Post cleaning
```{r, echo = FALSE}
d<-data.table(d)
demographics=data.table(d[,.(English_as_primary,Male,Household_Income,Speaks_German,Speaks_French,Drink_Frequency,Cab_Preference,Purchase_Frequency)])

stargazer(d[,.(Guess_the_Cost,Purchase_50,Purchase_35,Purchase_20)],type="text",title = "Output variables, Pre-Cleaning")
hist(d$Guess_the_Cost[d$Guess_the_Cost],main="Guess the cost, Post-Cleaning",breaks=100,xlab= "Guessed cost")

par(mfrow=c(3,3))
#look at randomization/tracks 
barplot(prop.table(table(d$Track)),main="Distribution across Tracks, Normalized",names.arg = c(-1:12))
#Important to note- 0 on barchart is N/A or attrition.
for (i in (1:length(names(demographics)))){
  #print(summary(demographics[,..i]))
  barplot(table(demographics[,..i]),
           main=names(demographics)[i]
          #,names.arg = demographics_labels[i]
          )}
```


Next, the team decided to  dive a bit deeper into the guess-the-cost results, to see whether a treatment effect was obvious through histograms. We evaluated histograms that were summarized at each level of our three treatment factors: 

```{r, echo = FALSE}
d<-data.table(complete_dedup)
#back to factors
d$French_Purchasing_Page <- factor(d$French_Purchasing_Page)
d$German_Purchasing_Page <- factor(d$German_Purchasing_Page)
d$US_Origin <-factor(d$US_Origin)
d$Long_Description <-factor(d$Long_Description)

#hist, includes all cleaned 
#hist(d$Guess_the_Cost,main="All conditions",xlab= "Guessed cost", xlim = c(0,100),breaks=1000)
#abline(v = mean(d$Guess_the_Cost), col = "blue")
#abline(v = median(d$Guess_the_Cost), col = "green")

#formats
par(mfrow=c(1,3))
#pin(c(2,2))
#d$French_Purchasing_Page<-factor(d$French_Purchasing_Page)
hist(d$Guess_the_Cost[d$French_Purchasing_Page == 1],main = "French format",xlab= "Guessed cost", xlim = c(0,100),breaks=1000)
abline(v = mean(d$Guess_the_Cost[d$French_Purchasing_Page == 1]), col = "blue")
abline(v = median(d$Guess_the_Cost[d$French_Purchasing_Page == 1]), col = "green")

hist(d$Guess_the_Cost[d$German_Purchasing_Page == 1], main = "German format",xlab= "Guessed cost", xlim = c(0,100),breaks=1000)
abline(v = mean(d$Guess_the_Cost[d$German_Purchasing_Page == 1]), col = "blue")
abline(v = median(d$Guess_the_Cost[d$German_Purchasing_Page == 1]), col = "green")

hist(d$Guess_the_Cost[d$German_Purchasing_Page == 0 & d$French_Purchasing_Page == 0 ],main = "English format", xlab= "Guessed cost", xlim = c(0,100),breaks=1000)
abline(v = median(d$Guess_the_Cost[d$German_Purchasing_Page == 0 & d$French_Purchasing_Page == 0]), col = "green")
abline(v = mean(d$Guess_the_Cost[d$German_Purchasing_Page == 0 & d$French_Purchasing_Page == 0]), col = "blue")

```

```{r, echo = FALSE}
#hist figures for origin, description, cleaned 
par(mfrow=c(2,2))

#Origins
d_chart=d$Guess_the_Cost[d$US_Origin == 1]
hist(d_chart,main = "US Origin Wine",xlab= "Guessed cost", xlim = c(0,100),breaks=1000)
abline(v = mean(d_chart), col = "blue")
abline(v = median(d_chart), col = "green")

d_chart=d$Guess_the_Cost[d$US_Origin == 0]
hist(d_chart,main = "French Origin Wine",xlab= "Guessed cost", xlim = c(0,100),breaks=1000)
abline(v = mean(d_chart), col = "blue")
abline(v = median(d_chart), col = "green")

#descriptions
d_chart=d$Guess_the_Cost[d$Long_Description == 0]
hist(d_chart,main = "Short Description",xlab= "Guessed cost", xlim = c(0,100),breaks=1000)
abline(v = mean(d_chart), col = "blue")
abline(v = median(d_chart), col = "green")

d_chart=d$Guess_the_Cost[d$Long_Description == 1]
hist(d_chart,main = "Long Description",xlab= "Guessed cost", xlim = c(0,100),breaks=1000)
abline(v = mean(d_chart), col = "blue")
abline(v = median(d_chart), col = "green")
```
From these historgrams of the answers, we don't observe a very apparent difference based on page lanugae, country of origin, or length of description. This is not to say that we don't have any treatment effect, it's that the effect that our treament have may be too small to appear in these visualization.

## Experimental Assumptions

Before beginning our modeling, the team proceeded to first validate our experimental assumptions, including conducting a covariance balance check, a check for potential differential attrition, and a check for potential heterogeneous treatment effects.

### Demographic/Covariate Balance Check  

For a covaraite balance check, the team regressed each treatment factor against our covariates. The goal was to use the F-statistic as an indicator of whether our covariates had any explanatory power on which treatment track a respondent was assigned to. For 3 treatment factors (Origin, Language Format, Description length) with 2, 3 and 2 levels respectively, the team ended up constructing four regressions that are listed below:


$${\scriptstyle 1)FrenchPage = \beta_0 + \beta_1Male+\beta_2Income + \beta_3EnglishPrimary+\beta_4DrinkFrequency+\beta_5CabPreference + \beta_6PurchaseFrequency+u} $$  
$$ {\scriptstyle 2)GermanPage = \beta_0 + \beta_1Male+\beta_2Income + \beta_3EnglishPrimary+\beta_4DrinkFrequency+\beta_5CabPreference + \beta_6PurchaseFrequency+u} $$  
$${\scriptstyle 3)USOrigin = \beta_0 + \beta_1Male+\beta_2Income + \beta_3EnglishPrimary+\beta_4DrinkFrequency+\beta_5CabPreference +
\beta_6PurchaseFrequency+u} $$  
$$ {\scriptstyle 4)LongDescription = \beta_0 + \beta_1Male+\beta_2Income + \beta_3EnglishPrimary+\beta_4DrinkFrequency+\beta_5CabPreference +
\beta_6PurchaseFrequency+u} $$  

In these regressions, Male, EnglighPrimary and CabPreference are binary variables. Income, Drink Frequency and Purchase Frequency represent factor vectors. Equations 1 and 2 were evaluated on subsets of our original data frame. For equation 1, the data frame only contained observations where the purchasing page was in English or French. For equation 2, the data frame only contained observations where the purchasing page was in English or German. After running all four of these regressions, the p-values for each of the F-statistics were much greater than 0.05. The team concludes that our covariate balance check passes, seeing that we cannot reject the null hypothesis that all beta values are zero for these regressions.
```{r, include = FALSE}
#d$Cab_Preference = factor(d$Cab_Preference)
#d$French_Purchasing_Page = factor(d$French_Purchasing_Page)
d$French_Purchasing_Page = as.numeric(d$French_Purchasing_Page)
d$German_Purchasing_Page = as.numeric(d$German_Purchasing_Page)
d$US_Origin = as.numeric(d$US_Origin)
d$Long_Description = as.numeric(d$Long_Description)


French_cb = lm(French_Purchasing_Page ~ Male + Household_Income + English_as_primary + Drink_Frequency + Cab_Preference + Purchase_Frequency, data = d[d$German_Purchasing_Page != 1,])

German_cb = lm(German_Purchasing_Page ~ Male + Household_Income + English_as_primary + Drink_Frequency + Cab_Preference + Purchase_Frequency, data = d[d$French_Purchasing_Page != 1,])

Origin_cb = lm(US_Origin ~ Male + Household_Income + English_as_primary + Drink_Frequency + Cab_Preference + Purchase_Frequency, data = d)

Description_cb = lm(Long_Description ~ Male + Household_Income + English_as_primary + Drink_Frequency + Cab_Preference + Purchase_Frequency, data = d)

stargazer(French_cb,German_cb,Origin_cb,Description_cb,align=TRUE,type="text", header = FALSE)

d<-data.table(complete_dedup)
```


### Attrition check

As mentioned earlier, our attrition is rather low for the entire study (~5%), so we are not very concerned about potential bias caused by potential differential attrition. However, we decided to do a check for completeness. Here, we used logistic regression (since whether or not finishes the survey is binary) to check whether being assigned to certain treamtent leads to more or less attrition than others.

```{r, echo = FALSE, results='asis'} 
OR = function(regr){
  return(exp(coef(regr)))
}

attrit = subset(d, Finished == 0)
#sum(attrit$French_Purchasing_Page == 1)
#sum(attrit$German_Purchasing_Page == 1)
#sum(attrit$French_Purchasing_Page == 0 & attrit$German_Purchasing_Page == 0)

#lm_attrit_format = lm(Finished~French_Purchasing_Page + German_Purchasing_Page, d)
#lm_attrit_origin = lm(Finished~ US_Origin, d)
#lm_attrit_descrip = lm(Finished~Long_Description, d)

## Logistic regression
LR_format <- glm (Finished ~ French_Purchasing_Page + German_Purchasing_Page, data=d, family = binomial)
LR_origin <- glm (Finished ~ US_Origin , data=d, family = binomial)
LR_descrip <- glm (Finished ~ Long_Description, data=d, family = binomial)

#stargazer(lm_attrit_format,lm_attrit_origin,lm_attrit_descrip, title="Attrition Regressions-OLS", align=TRUE, type="text")

stargazer(LR_format, LR_origin, LR_descrip, coef = list(OR(LR_format), OR(LR_origin), OR(LR_descrip)), title="Attrition Regressions-Logistic with Odds Ratio", p.auto = FALSE, type = "latex", column.sep.width = "1pt", header = FALSE, font.size = "scriptsize")
```

For the regression table above, we see that logistic regression suggests that seeing French or German purchasing page made people more likely to drop off. In other words, we do find evidence for differential attrition (this statistically significant finding was replicated with linear regression as well). However, since our attrition rate is very low, even if the 5% that dropped out have very extreme outcomes, they are unlikely to bias our estimate too significantly. Therefore, we are not very concerned about this effect. Additionally, when conducting a differential attrition check for our validation study, the team was not able to replicate the finding, and found no statistically significant effects. 

### Heterogeneous treatment effects
We tested for heterogeneous treatment effects using regression for those who answered "Never" for their wine purchasing behavior. We hypothesized that people that never purchase wine may have a rather un-firm value perception of wine, and may be influenced by our treatment more easily. In our test, we found a significance in the interaction term between never-pruchase and German language, which does suggest the presence of HTE. However, this effect was not observed in the second launch.

```{r, include = FALSE}
d<-data.table(complete_dedup)
d$Never_purchased<-rep(0,length(d$Purchase_Frequency))
d[Purchase_Frequency==5,Never_purchased := 1]
#need character format from sharad's code:

d$French_Purchasing_Page = as.numeric(as.character(d$French_Purchasing_Page))
d$German_Purchasing_Page = as.numeric(as.character(d$German_Purchasing_Page))
d$US_Origin = as.numeric(as.character(d$US_Origin))
d$Long_Description = as.numeric(as.character(d$Long_Description))

rlm.HTE_format<-rlm(Guess_the_Cost~ French_Purchasing_Page + German_Purchasing_Page+Never_purchased + French_Purchasing_Page*Never_purchased + German_Purchasing_Page * Never_purchased,d)

rlm.HTE_origin<-rlm(Guess_the_Cost~  US_Origin +Never_purchased+ Never_purchased * US_Origin,d)
rlm.HTE_description<-rlm(Guess_the_Cost~ Long_Description+Never_purchased + Never_purchased * Long_Description,d)

lm.cv_HTE<-lm(French_Purchasing_Page~Never_purchased,d)
summary(lm.cv_HTE)

```

```{r, echo = FALSE, results='asis'}
stargazer(rlm.HTE_format,rlm.HTE_origin,rlm.HTE_description, type = "latex", column.sep.width = "1pt", title =  'HTE', header = FALSE, font.size = "scriptsize")

```


# Modeling

##1 RI with limit on guesses
Our first attempt to isolate a treatment effect using random inference and the sharp null hypothesis. By nature of RI, in which a high outlier may swing the entire treatment- or control-assigned group, we limited the guesses to the bottom 95%. Using the reduced outcome set, we tested the simple treatments and our specified interaction (same format*origin) hypotheses.

```{r, include=FALSE}
##Dedupping the factorized table to use for RI
#Convert data to omit na's (attrition)

d<- d_factor[!is.na(d_factor$Track),]
# Counting occurances of IP address and Geo-codes
d$geo_code <- paste(d$LocationLatitude, d$LocationLongitude)
dt = data.table(d)
dt[, `freq_geo` := .N, by = geo_code]
dt[, `freq_ip` := .N, by = IPAddress]
d_factor<- dt[dt$freq_ip==1 & dt$freq_geo==1]
d<- d_factor[d_factor$freq_ip==1 & d_factor$freq_geo==1]

```

```{r, echo = FALSE}
DT<-data.table(d)
#for RI, only using the bottom 95percent
boxplot(DT$Guess_the_Cost,horizontal = TRUE)
title("Boxplot for Guesses including upper outliers")
DT_95<-DT[Guess_the_Cost < quantile(DT$Guess_the_Cost,.95)]
boxplot(DT_95$Guess_the_Cost,horizontal = TRUE)
title("Boxplot for Guesses, bottom 95 percent")
```


```{r, include=FALSE}
#Set up RI for 0-95 of guesses
set.seed(0)

#2. calculate ate function
calculate_ate<- function(d=RI_DT,outcome,feature){
control<-outcome[feature==0]
treatment<-outcome[feature==1]
ate<-mean(treatment)-mean(control)
return(ate)
}

#3. Assume sharp null
#make new table with randomized features (input)
random_it<-function(d,outcome,feature){
d_new<-data.frame(X=sample(feature),Y=outcome)
ate<-calculate_ate(d_new,d_new$Y,d_new$X)
return(ate)
}
#random_it(RI_DT,RI_outcome,RI_feature)

#4. rinse and repeat
rep_rand_Sharpnull<-function(d,NITERS,outcome,feature){
ate_list<-rep(NA, NITERS)
  for (i in (1:NITERS)){
    ate_list[i]<- random_it(d,outcome,feature)
  }
return(ate_list)
}

RI_chartoutput<-function(DT=RI_DT, outcome=RI_outcome, RI_feature,feature_title){
  ate_obsv<-calculate_ate(DT,outcome,RI_feature)
  ate_list<-rep_rand_Sharpnull(d,NITERS=1000,outcome,RI_feature)
  p_value<-mean(ate_obsv<=ate_list)
  hist(ate_list,main = sprintf("Histogram under Sharp Null \n %s: P-value %f",feature_title,p_value))
  abline(v = ate_obsv, col = "blue")
  }

```

```{r, echo = FALSE}
#Formats
par(mfrow=c(2,1))
RI_chartoutput(DT_95,DT_95$Guess_the_Cost,DT_95$French_Purchasing_Page,"French Purchasing Page")
RI_chartoutput(DT_95,DT_95$Guess_the_Cost,DT_95$German_Purchasing_Page,"German Purchasing Page")
#Wine origin, Description
par(mfrow=c(2,1))
RI_chartoutput(DT_95,DT_95$Guess_the_Cost,DT_95$US_Origin,"US Origin (vs French)")
RI_chartoutput(DT_95,DT_95$Guess_the_Cost,DT_95$Long_Description,"Long Description")

```

The results from our Random Inference attemp is in the 4 tables above. Here we see no significant result for all of the 4 treatments (French purchasing page, German purchasing page, US origin, and Long description. Therefore, sadly we could not reject the sharp null hypothese that the treatment effect is zero for all respondents. On to linear regression.

##2 Model Evaluation Strategy

The first outcome of interest was respondents guesses for the price of a wine. Initially, the team wanted to see how standard OLS regressions would perform with our data. We were not very optimistic based on the considerable variance observed from our earlier descriptive statistics. Preliminary models that we built reinforced this concern. The table below contains simple OLS models where we used only the treatment variables as predictors. As you can see from the table below, the most simplistic of regressions were returning large standard errors.  

```{r, echo=FALSE, results= 'asis'}
#basic and covariate models
lm.basic_format<-lm(Guess_the_Cost~French_Purchasing_Page+German_Purchasing_Page,d)
lm.basic_origin<-lm(Guess_the_Cost~US_Origin,d)
lm.basic_description<-lm(Guess_the_Cost~Long_Description,d)
lm.cv_format<-lm(Guess_the_Cost~French_Purchasing_Page + German_Purchasing_Page + Male + Household_Income + English_as_primary  + Drink_Frequency + Cab_Preference + Purchase_Frequency,DT)
lm.cv_origin<-lm(Guess_the_Cost~US_Origin + Male + Household_Income + English_as_primary  + Drink_Frequency + Cab_Preference + Purchase_Frequency,DT)
lm.cv_description<-lm(Guess_the_Cost~Long_Description + Male + Household_Income + English_as_primary  + Drink_Frequency + Cab_Preference + Purchase_Frequency,DT)

lm.basic_all<-lm(Guess_the_Cost ~ French_Purchasing_Page+German_Purchasing_Page+US_Origin+Long_Description,d)
lm.cv_all<-lm(Guess_the_Cost~French_Purchasing_Page+German_Purchasing_Page+US_Origin+Long_Description + Male + Household_Income + English_as_primary  + Drink_Frequency + Cab_Preference + Purchase_Frequency,DT)

stargazer(lm.basic_format,lm.basic_origin,lm.basic_description,lm.basic_all, column.sep.width = "1pt" , type="latex", font.size = "scriptsize", header = F, title = "Standard OLS Regressions")
#stargazer(lm.cv_format,lm.cv_origin,lm.cv_description,lm.cv_all,align=TRUE,type="text")
```

```{r, include = FALSE}
d$same_format_origin<-rep(0,length(d$French_Purchasing_Page))
d[French_Purchasing_Page==1 & US_Origin==0,same_format_origin :=1]
d[German_Purchasing_Page==0 & French_Purchasing_Page==0 & US_Origin==1,same_format_origin :=1]
#French layout+french wine
d$French_format_wine<-rep(0,length(d$French_Purchasing_Page))
d[French_Purchasing_Page==1 & US_Origin==0,"French_format_wine" :=1]
#English layout + US wine
d$English_format_wine<-rep(0,length(d$French_Purchasing_Page))
d[German_Purchasing_Page==0 & French_Purchasing_Page==0 & US_Origin==1,"English_format_wine" :=1]

#models
lm.same<-lm(Guess_the_Cost ~  same_format_origin + Male + Household_Income + English_as_primary + Drink_Frequency + Cab_Preference + Purchase_Frequency ,d)
#summary(lm.same)
lm.same_indiv<-lm(d$Guess_the_Cost ~ French_format_wine + English_format_wine + Male + Household_Income + English_as_primary + Drink_Frequency + Cab_Preference + Purchase_Frequency,d)
stargazer(lm.same,lm.same_indiv,type = "latex", column.sep.width = "1pt", header = FALSE, font.size = "scriptsize")

```

After running a few more models with covariates, the team realized that these large standard errors were going to be an issue in all OLS regression models we run. Therefore, the team decided to go with a different strategy to evaluate a treatment effect for this outcome, and that was through Robust Linear Models. The idea of this model is to weigh observations differently based on how high their residuals are. We viewed this a compromise between eliminating extreme guesses entirely from the analysis and treating all price guesses equally like we would in OLS regression.

For the second outcome of interest, the team decided to evaluate three different outcomes of willingness to purchase. Since our outcomes for each of these 3 price points are binary, we decided to use binomial logistic regression as our method of analysis. With regards to the output of logistic regression models, the team chose to exponentiate the coefficients, as we could then interpret effects as the odds of purchasing the wine at a certain price point (odds ratio). The team also considered using an ordinal logistic regression instead, and looking at maximum pricepoint at which individuals would purchase; however we did not believe the levels of our willingness to purchase outcome met the conditions of the  proportional odds assumptions. Therefore we thought it would be more prudent to use binary logistic regression with three separate outcomes. 

## Model Matrix  

![Model summary](Model Summary.png)

The matrix above describes the different RLM and Logistic regression models we ran on our training set. Each column describes the details of predictors we used in the model, and each row describes the outcome variable for the model and whether or not covariates were included:

1) The left-hand side of the table pertains to our most simple models, where we only used the treatment variables as predictors without any interactions  
2) For Isolated Interactions we only examined the effect of treatment where the wine origin and purchasing page format are the same. For example the effect of French origin-French language treatment, or US origin-English language treatment.  
3) For pairwise interaction models, we would include only 2 of 3 treatments per model and observe their dynamic. For example, one model could include language format and description length as predictors, as well as their interaction, regardless of the origin of the wine.
4) For the fully saturated models, we included each of the treatment variables, and all pairwise and three-prong intereactions between treatment.  

The exact model equations can be found in Appendix A.


On the left-hand side of the table there is a box stating, “No Statistically Significant Findings” and that pertains to the most simplistic regressions we ran. We found significance in Now with these more detailed models, the team started to uncover some findings that are significant at 10% level and others at the 5% level.  For brevity I want to direct your attention to the two findings we found most interesting. The first was the positive effect of the interaction between French Format and French Origin on the willingness to purchase at $20.  This made a lot of sense to us, considering that adding French language to describe a French wine could possibly attribute more authenticity to the wine. The second model of interest was the fully saturated model where there was a negative effect of the interaction between German Format and Long Description on the willingness to purchase at $35. But when you add US origin as a third piece to that interaction, the effect switched 




##RLM

#Individual Treatments- RLM regressions with no covariates
```{r}
#PRE-RLM data formatting
d$French_Purchasing_Page<-factor(d$French_Purchasing_Page)
d$German_Purchasing_Page<-factor(d$German_Purchasing_Page)
d$US_Origin <- factor(d$US_Origin)
d$Long_Description <- factor(d$Long_Description)

d$Male = as.character(d$Male)
d$Household_Income = as.character(d$Household_Income)
d$Cab_Preference = as.character(d$Cab_Preference)
d$English_as_primary = as.character(d$English_as_primary)
d$Drink_Frequency = as.character(d$Drink_Frequency)
d$Purchase_Frequency = as.character(d$Purchase_Frequency)
```

**No statistical significance in treatments**
```{r}
rlm.format<-rlm(Guess_the_Cost~French_Purchasing_Page + German_Purchasing_Page, d)
rlm.origin<-rlm(Guess_the_Cost~US_Origin,d)
rlm.description<-rlm(Guess_the_Cost~Long_Description,d)
rlm.combined<-rlm(Guess_the_Cost~French_Purchasing_Page + German_Purchasing_Page + US_Origin + Long_Description,d)


stargazer(rlm.combined,rlm.format,rlm.origin,rlm.description,title="RLM Basic regressions- no covariates", type = "latex", column.sep.width = "1pt", header = FALSE, font.size = "scriptsize")
```

#Individual Treatments- RLM regressions with covariates

**No statistical significance in treatments**
```{r}
Factors_omit=c("Household_Income","Household_Income2","Household_Income3","Household_Income4","Household_Income5","Household_Income6","Household_Income7","Drink_Frequency","Drink_Frequency2","Drink_Frequency3","Drink_Frequency4","Drink_Frequency5","Purchase_Frequency","Purchase_Frequency2","Purchase_Frequency3","Purchase_Frequency4","Purchase_Frequency5")


rlm.cv<-rlm(Guess_the_Cost~French_Purchasing_Page + German_Purchasing_Page + US_Origin + Long_Description +  Male + Household_Income + English_as_primary  + Drink_Frequency + Cab_Preference + Purchase_Frequency, d)

rlm.cv1<-rlm(Guess_the_Cost~French_Purchasing_Page + German_Purchasing_Page + Male + Household_Income + English_as_primary  + Drink_Frequency + Cab_Preference + Purchase_Frequency, d)

rlm.cv2 <- rlm(Guess_the_Cost~ US_Origin + Male + Household_Income + English_as_primary  + Drink_Frequency + Cab_Preference + Purchase_Frequency,d)

rlm.cv3 <- rlm(Guess_the_Cost~ Long_Description + Male + Household_Income + English_as_primary  + Drink_Frequency + Cab_Preference + Purchase_Frequency,d)

stargazer(rlm.cv, rlm.cv1, rlm.cv2, rlm.cv3, align=TRUE, title = "RLM Basic regressions- with covariates (factors omitted from table)",omit=Factors_omit, type = "latex", column.sep.width = "1pt", header = FALSE, font.size = "scriptsize")
```
####Comments on improvement, std error etc
###testing interaction hypothesis
```{r}
#Interactions - Same Format/Origin without covariates
#**No statistical significance for treatment**
#models
rlm.same<-rlm(Guess_the_Cost ~ same_format_origin, d)
#summary(lm.same)

rlm.same_indiv<-rlm(Guess_the_Cost ~ French_format_wine + English_format_wine, d)

stargazer(rlm.same,rlm.same_indiv, type = "latex", column.sep.width = "1pt", header = FALSE, font.size = "scriptsize")
```



#Saturated

**No statistical significance for treatments**
```{r}
d$Track<-factor(d$Track)

rlm.saturated_cv<-rlm(Guess_the_Cost~ Track + Male + Household_Income + English_as_primary + Drink_Frequency + Cab_Preference + Purchase_Frequency,d)

```

**TrackFrench_English_long  significant at 10% level**
```{r}

rlm.saturated<-rlm(Guess_the_Cost~ Track,d)

stargazer(rlm.saturated_cv,rlm.saturated, align=TRUE,type="text", title = "RLM regressions- Track",omit=Factors_omit)
```
###Note that our SD's decreased w/o covariates

#Full Interactions without covariates

**Statistical Significance at the 10% level for Long Description**

```{r}
rlm.interactions<-rlm(Guess_the_Cost~ French_Purchasing_Page + German_Purchasing_Page + US_Origin + Long_Description + French_Purchasing_Page * US_Origin + German_Purchasing_Page * US_Origin + French_Purchasing_Page * Long_Description + German_Purchasing_Page * Long_Description + US_Origin * Long_Description + French_Purchasing_Page * US_Origin * Long_Description + German_Purchasing_Page* US_Origin * Long_Description,d)

stargazer(rlm.interactions, align=TRUE,type="text", title = "RLM regression- with interactions")
```

#Interaction Pairs without covariates

**Statistical Significance at the 5% level for Long Description (regression without language format)**
**Statistical Significance at the 10% level for interaction term Long Description with US Origin (regression without language format)**
**Statistical Significance at the 10% level for German Purchasing Page (regression without origin)**

```{r}
rlm.i1<-rlm(Guess_the_Cost~ French_Purchasing_Page + German_Purchasing_Page + US_Origin + French_Purchasing_Page * US_Origin + German_Purchasing_Page * US_Origin,d)

rlm.i2<-rlm(Guess_the_Cost~ French_Purchasing_Page + German_Purchasing_Page + Long_Description + French_Purchasing_Page * Long_Description + German_Purchasing_Page * Long_Description,d)

rlm.i3<- rlm(Guess_the_Cost~ US_Origin + Long_Description + US_Origin * Long_Description, d)

stargazer(rlm.i1, rlm.i2, rlm.i3, align=TRUE,type="text", title = "RLM regression- with interactions")
```

###PURCHASE LIKELIHOOD - MULTINOMIAL LOGISTIC REGRESSION

**Using multinomial instead of ordinal due to the proportional odds assumption: The proportional odds assumption means that for each term included in the model, the 'slope' estimate between each pair of outcomes across two response levels are assumed to be the same regardless of which partition we consider**

```{r}
d$max_purchase[!is.na(d$Purchase_50)] = "$0"
d[Purchase_50==1,max_purchase:="$50"]
d[Purchase_35==1 & Purchase_50 == 0,max_purchase:="$35"]
d[Purchase_20==1 & Purchase_50 == 0 & Purchase_35 == 0,max_purchase:="$20"]
d$max_purchase = as.factor(d$max_purchase)
d$max = relevel(d$max_purchase, ref = "$0") 


p_val = function(regr){
z <- summary(regr)$coefficients/summary(regr)$standard.errors
return((1 - pnorm(abs(z), 0, 1)) * 2)
}

risk_ratio = function(regr){
z <- summary(regr)$coefficients/summary(regr)$standard.errors
exp(coef(regr))
}

```

####Trying binary logit models instead

```{r}
#Odds Ratio
OR = function(regr){
  return(exp(coef(regr)))
}
```

##Log Regression Page Format - No covariates
**No statistical significnace**
```{r}
glm.format_50<-glm(Purchase_50~French_Purchasing_Page + German_Purchasing_Page,family = binomial, d)
glm.format_35<-glm(Purchase_35~French_Purchasing_Page + German_Purchasing_Page,family = binomial, d)
glm.format_20<-glm(Purchase_20~French_Purchasing_Page + German_Purchasing_Page, family = binomial,d)


#rlm.origin<-rlm(Guess_the_Cost~US_Origin,d)
#rlm.description<-rlm(Guess_the_Cost~Long_Description,d)
#rlm.combined<-rlm(Guess_the_Cost~French_Purchasing_Page + German_Purchasing_Page + US_Origin + Long_Description,d)

stargazer(glm.format_50,glm.format_35,glm.format_20, coef = list(OR(glm.format_50), OR(glm.format_35), OR(glm.format_20)), p.auto = FALSE, title="Log Regression Page Format- Risk Ratios with no covariates", align=TRUE, type="text")
```

##Log Regression Origin - No Covariates
**No statistical significance**
```{r}
glm.origin_50<-glm(Purchase_50~ US_Origin,family = binomial, d)
glm.origin_35<-glm(Purchase_35~ US_Origin,family = binomial, d)
glm.origin_20<-glm(Purchase_20~ US_Origin, family = binomial,d)


#rlm.origin<-rlm(Guess_the_Cost~US_Origin,d)
#rlm.description<-rlm(Guess_the_Cost~Long_Description,d)
#rlm.combined<-rlm(Guess_the_Cost~French_Purchasing_Page + German_Purchasing_Page + US_Origin + Long_Description,d)

stargazer(glm.origin_50,glm.origin_35,glm.origin_20, coef = list(OR(glm.origin_50), OR(glm.origin_35), OR(glm.origin_20)), p.auto = FALSE, title="Log Regression Wine Origin- Risk Ratios with no covariates", align=TRUE, type="text")
```

##Log Regression Description - No covariates
**No statistical significance**
```{r}
glm.desc_50<-glm(Purchase_50~ Long_Description, family = binomial, d)
glm.desc_35<-glm(Purchase_35~ Long_Description, family = binomial, d)
glm.desc_20<-glm(Purchase_20~ Long_Description, family = binomial,d)


#rlm.origin<-rlm(Guess_the_Cost~US_Origin,d)
#rlm.description<-rlm(Guess_the_Cost~Long_Description,d)
#rlm.combined<-rlm(Guess_the_Cost~French_Purchasing_Page + German_Purchasing_Page + US_Origin + Long_Description,d)

stargazer(glm.desc_50,glm.desc_35,glm.desc_20, coef = list(OR(glm.desc_50), OR(glm.desc_35), OR(glm.desc_20)), p.auto = FALSE, title="Log Regression Wine Description- Risk Ratios with no covariates", align=TRUE, type="text")
```


## Insert data from 2nd launch, and 2 test models from 2nd launch & verbiage




# Conclusion
The one significant finding that we have that persisted in both main and validation study (although only at 10%) is the positive impact of French wine in French purchasing page. We found that this combintion drives up purchase likelihood at $20. Advertsing in French for a entry-level French wine may be a good idea for businesses (further validation needed).

Having gone through the study, we learned a lot about experimental design, and if we were to do the study again, we would improve oru design in the following ways:

1. Introducing a baseline question in main study. The milk-cost baseline question helped us control for some outlier answer, especially those caused by currency perception. Having this in the main study would definitely help in our modeling process.

2. Limiting geographical location of respondents. In mechanical Turk, we did not limit to only US respondents, for fear that we may not achieve our ambitious sample on time. If time permits, this limitation would be useful because it will then eliminate currency bias.

3. Increasing sample size, especially in validation study. The large variation we see in responses also calls for large sample sizes. If we have enough time and resources, gathering a larger sample helps increase statistical power.

4. Changing treatment dosage. For this treatment, we used the "maximum dosage possible" by translating everything into the treatment language. This may have overwhelmed or confused respondents, which lead to some differential attrition. In a future study, it may be wise to reduce the treatment dosage to a more manageable way (e.g. only translating flavor profile and tagline),

5. Redesigning the guess-the-cost question. We liked the question because it is completely unanchored, which allowed us to find psychological price thresholds (like $20). however, for nature of this question is outlier prone. In a new study, we may consider changing this question type, or at least putting a maximum answer threshold on guess-the-cost questions implicitly, without anchoring respondents. Having an implicit max price (the highest price imaginable for a wine, say 300), where only people that entered more than the value would see, may help us reduce the magnitude of outliers.
