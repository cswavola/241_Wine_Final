---
title: "Final_Regressions"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath("./"))
```


```{r, echo=FALSE}
d = read.xlsx('Final_241_Data_Shared.xlsx')
#d = d[d$Guess_the_Cost <= 2000,]
# load packages 
library(data.table)
library(dplyr)
library(tidyr)
library(foreign)
library(lmtest)
library(sandwich)
library(multiwayvcov)
```


#1. Functions to use
```{r}
standard_conf_int_95 = function(regression, n){
  r = coeftest(regression, vcovHC(regression))
  upper_bound = r[n,1] + 1.96* r[n, 2] 
  lower_bound = r[n,1] -1.96* r[n, 2]
  return (c(lower_bound,upper_bound))
}
```
#deduping/cleaning
```{r deduping}
library(data.table)
# Cleaning out incomplete responses before treament exposures
d <- d[!is.na(d$Track),]
#summary(d)
# Counting occurances of IP address and Geo-codes
d$geo_code <- paste(d$LocationLatitude, d$LocationLongitude)
dt = data.table(d)
dt[, `freq_geo` := .N, by = geo_code]
dt[, `freq_ip` := .N, by = IPAddress]
#class(as.data.frame(dt))
#dt
## Creatig 2 dedupped dataset, one with complete dedup where we do not allow any duplicates in IP or Geo code, and one where we exclude IP dupes, but allow up to 5 dups per geo code
complete_dedup<- dt[dt$freq_ip==1 & dt$freq_geo==1]
#nrow(complete_dedup)
five_or_less_dedup <- dt[dt$freq_ip==1 & dt$freq_geo<=5]
#nrow(five_or_less_dedup)
d<-five_or_less_dedup
```

##Summary/Initial Plots
###All guesses, including high outliers
```{r}
hist(d$Guess_the_Cost[d$Guess_the_Cost<1500],main="All Guesses",breaks=100,xlab= "Guessed cost")
```

```{r}
#hist, includes all cleaned 
hist(d$Guess_the_Cost,main="All conditions",xlab= "Guessed cost", xlim = c(0,100),breaks=1000)
abline(v = mean(d$Guess_the_Cost), col = "blue")
abline(v = median(d$Guess_the_Cost), col = "green")

#formats
par(mfrow=c(1,3),pin=c(2,2))

hist(d$Guess_the_Cost[d$French_Purchasing_Page == 1],main = "French format",xlab= "Guessed cost", xlim = c(0,100),breaks=1000)
abline(v = mean(d$Guess_the_Cost[d$French_Purchasing_Page == 1]), col = "blue")
abline(v = median(d$Guess_the_Cost[d$French_Purchasing_Page == 1]), col = "green")

hist(d$Guess_the_Cost[d$German_Purchasing_Page == 1],main = "German format",xlab= "Guessed cost", xlim = c(0,100),breaks=1000)
abline(v = mean(d$Guess_the_Cost[d$German_Purchasing_Page == 1]), col = "blue")
abline(v = median(d$Guess_the_Cost[d$German_Purchasing_Page == 1]), col = "green")

hist(d$Guess_the_Cost[d$German_Purchasing_Page == 0 & d$French_Purchasing_Page == 0 ],main = "English format", xlab= "Guessed cost", xlim = c(0,100),breaks=1000)
abline(v = median(d$Guess_the_Cost[d$German_Purchasing_Page == 0 & d$French_Purchasing_Page == 0]), col = "green")
abline(v = mean(d$Guess_the_Cost[d$German_Purchasing_Page == 0 & d$French_Purchasing_Page == 0]), col = "blue")

```
```{r}
#hist figures for origin, description, cleaned 
par(mfrow=c(2,2))

#Origins
d_chart=d$Guess_the_Cost[d$US_Origin == 1]
hist(d_chart,main = "US Origin Wine",xlab= "Guessed cost", xlim = c(0,100),breaks=1000)
abline(v = mean(d_chart), col = "blue")
abline(v = median(d_chart), col = "green")

d_chart=d$Guess_the_Cost[d$US_Origin == 0]
hist(d_chart,main = "French Origin Wine",xlab= "Guessed cost", xlim = c(0,100),breaks=1000)
abline(v = mean(d_chart), col = "blue")
abline(v = median(d_chart), col = "green")

#descriptions
d_chart=d$Guess_the_Cost[d$Long_Description == 0]
hist(d_chart,main = "Short Description",xlab= "Guessed cost", xlim = c(0,100),breaks=1000)
abline(v = mean(d_chart), col = "blue")
abline(v = median(d_chart), col = "green")

d_chart=d$Guess_the_Cost[d$Long_Description == 1]
hist(d_chart,main = "Long Description",xlab= "Guessed cost", xlim = c(0,100),breaks=1000)
abline(v = mean(d_chart), col = "blue")
abline(v = median(d_chart), col = "green")
```

```{r}
#with outliers
r = lm(Guess_the_Cost~French_Purchasing_Page + German_Purchasing_Page, data = d)

coeftest(r, vcovHC(r))

(French_95 = standard_conf_int_95(r,'French_Purchasing_Page'))
(German_95 = standard_conf_int_95(r,'German_Purchasing_Page'))

sd(d$Guess_the_Cost[d$French_Purchasing_Page == 1])
sd(d$Guess_the_Cost[d$German_Purchasing_Page == 1])
sd(d$Guess_the_Cost[d$German_Purchasing_Page == 0 & d$French_Purchasing_Page ==0])
```





```{r}
(French_effect = cohen.d(Guess_the_Cost~French_Purchasing_Page, data = d[d$German_Purchasing_Page != 1,]))

(German_effect = cohen.d(Guess_the_Cost~German_Purchasing_Page, data = d[d$French_Purchasing_Page != 1,]))
```
**Effect size is very small for both French and German effect**

```{r}
pwr.t2n.test(n1 = NROW(d[d$German_Purchasing_Page != 1 & d$French_Purchasing_Page != 1,]), n2 = NROW(d[d$French_Purchasing_Page == 1,]), d = .045, sig.level = .05)

pwr.t2n.test(n1 = NROW(d[d$German_Purchasing_Page != 1 & d$French_Purchasing_Page != 1,]), n2 = NROW(d[d$German_Purchasing_Page == 1,]), d = 0.037, sig.level = .05)


```

**Due to the high variance in our Guess the Price outcome, our power for detecting an effect between French and English language was 9.8%, and our power for detecting an effect between German and English language was 8.22%.**

**Gpower says effect size f is 0.022**

```{r}
pwr.1way(k=3, n=400, f=0.1, alpha=0.05) #Aiming for effect size of 0.1 - Power of 88%

pwr.1way(k=3, n=400, f=0.022, alpha=0.05) #Got effect size of 0.022 - Power of 9.6%
```

```{r}
r = lm(Guess_the_Cost~French_Purchasing_Page + German_Purchasing_Page + US_Origin + French_Purchasing_Page * US_Origin + German_Purchasing_Page*US_Origin, data = d)

coeftest(r, vcovHC(r))
```

```{r}
r = lm(Guess_the_Cost~French_Purchasing_Page + German_Purchasing_Page + US_Origin + French_Purchasing_Page * Long_Description + German_Purchasing_Page*Long_Description, data = d)

coeftest(r, vcovHC(r))
```
```{r}
French_cb = lm(French_Purchasing_Page ~ Male + Household_Income + English_as_primary + Drink_Frequency + Cab_Preference + Purchase_Frequency, data = d[d$German_Purchasing_Page != 1,])

summary(French_cb)

German_cb = lm(German_Purchasing_Page ~ Male + Household_Income + English_as_primary + Drink_Frequency + Cab_Preference + Purchase_Frequency, data = d[d$French_Purchasing_Page != 1,])

summary(German_cb)

French_German_cb = lm(French_Purchasing_Page ~ Male + Household_Income + English_as_primary + Drink_Frequency + Cab_Preference + Purchase_Frequency, data = d[d$French_Purchasing_Page == 1 | d$German_Purchasing_Page == 1 ,])

summary(French_German_cb)
```

**p-values greater than .05 for all 3 regressions --> Covariate balance check shows that we cannot reject the null hypothesis that our covariates have no predictive power over our treatment variable**

```{r}
r = lm(Purchase_50 ~French_Purchasing_Page + German_Purchasing_Page, data = d)
coeftest(r, vcovHC(r))

```

```{r}
r = lm(Purchase_35 ~French_Purchasing_Page + German_Purchasing_Page, data = d)
coeftest(r, vcovHC(r))
```

```{r}
r = lm(Purchase_20 ~French_Purchasing_Page + German_Purchasing_Page, data = d)
coeftest(r, vcovHC(r))
```


```{r}

```

